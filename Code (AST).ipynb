{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#packages needed\n",
    "%pip install torch\n",
    "%pip install wget\n",
    "%pip install timm==0.4.5\n",
    "%pip install librosa\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a Fixed Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set the seed for PyTorch CPU and GPU (if available)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # For GPU (if available)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "    # Ensure deterministic behavior (important for reproducibility)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable auto-tuning of algorithms\n",
    "\n",
    "# Set a fixed random seed\n",
    "set_random_seed(42)  # You can replace 42 with any integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About AST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=108\n",
      "torch.Size([10, 20])\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "from ast_models import ASTModel \n",
    "# download pretrained model in this directory\n",
    "os.environ['TORCH_HOME'] = '../pretrained_models'  \n",
    "# assume each input spectrogram has 100 time frames\n",
    "input_tdim = 100\n",
    "label_dim = NUM_CLASSES\n",
    "# create a pseudo input: a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins \n",
    "test_input = torch.rand([10, input_tdim, 128]) \n",
    "# create an AST model\n",
    "ast_mdl = ASTModel(label_dim=label_dim, input_tdim=input_tdim, imagenet_pretrain=True)\n",
    "test_output = ast_mdl(test_input) \n",
    "# output should be in shape [10, 20], i.e., 10 samples, each with prediction of 20 classes. \n",
    "print(test_output.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account for longest audio in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max time steps: 688\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def find_max_time_step(json_file, sr=16000):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "    \n",
    "    max_time_steps = 0\n",
    "    for item in data:\n",
    "        audio_path = item['wav']\n",
    "        \n",
    "        # Load audio file and compute the mel spectrogram\n",
    "        y, _ = librosa.load(audio_path, sr=sr)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        \n",
    "        # Get the number of time frames (time steps)\n",
    "        time_steps = mel_spectrogram.shape[1]\n",
    "        \n",
    "        # Update max_time_steps if current audio has more time steps\n",
    "        max_time_steps = max(max_time_steps, time_steps)\n",
    "    \n",
    "    return max_time_steps\n",
    "\n",
    "# Example usage\n",
    "json_file = \"data_colab.json\" \n",
    "max_time_steps = find_max_time_step(json_file)\n",
    "print(f\"Max time steps: {max_time_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class ViolinAudioDataset(Dataset):\n",
    "    def __init__(self, json_file, num_classes=NUM_CLASSES, sr=16000, max_time_steps=688):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)[\"data\"]\n",
    "        self.num_classes = num_classes\n",
    "        self.sr = sr\n",
    "        self.max_time_steps = max_time_steps  # The fixed time steps for all spectrograms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.data[idx]['wav']\n",
    "        labels = self.data[idx]['labels']\n",
    "        \n",
    "        # Convert the comma-separated labels string into a list of integers\n",
    "        label_list = list(map(lambda x: int(x) - 1, labels.split(','))) if isinstance(labels, str) else [label - 1 for label in labels]\n",
    "        \n",
    "        # Convert labels to a multi-hot encoding vector\n",
    "        label_vector = np.zeros(self.num_classes, dtype=np.float32)\n",
    "        label_vector[label_list] = 1\n",
    "        \n",
    "        # Load and preprocess the audio\n",
    "        y, _ = librosa.load(audio_path, sr=self.sr)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=128)\n",
    "        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        \n",
    "        # Pad or truncate to max_time_steps\n",
    "        if mel_spectrogram_db.shape[1] < self.max_time_steps:\n",
    "            padding = self.max_time_steps - mel_spectrogram_db.shape[1]\n",
    "            mel_spectrogram_db = np.pad(mel_spectrogram_db, ((0, 0), (0, padding)), mode='constant')\n",
    "        else:\n",
    "            mel_spectrogram_db = mel_spectrogram_db[:, :self.max_time_steps]\n",
    "        \n",
    "        return torch.tensor(mel_spectrogram_db, dtype=torch.float32), torch.tensor(label_vector)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Debug: print the shape of the input tensor\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        # Ensure positional embeddings align with the input\n",
    "        if x.shape[1] != self.pos_embed.size(1):\n",
    "            print(f\"Resizing positional embeddings: current {self.pos_embed.size(1)}, expected {x.shape[1]}\")\n",
    "            self.pos_embed = torch.nn.Parameter(\n",
    "                torch.zeros(1, x.shape[1], self.pos_embed.size(-1))  # Dynamically adjust\n",
    "            )\n",
    "\n",
    "        # Debug: print the shape of positional embeddings\n",
    "        print(f\"Positional embeddings shape: {self.pos_embed.shape}\")\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Debug: print the shape after adding positional embeddings\n",
    "        print(f\"Shape after adding positional embeddings: {x.shape}\")\n",
    "\n",
    "        # Pass through the transformer or other layers\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Debug: print the final output shape\n",
    "        print(f\"Output shape: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Debug Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suspect the model is adding its own dimensions and messing shit up, \n",
    "#so explicitly defined forward function to print debug statements\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  # This imports the neural network module\n",
    "\n",
    "class ASTModel(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(ASTModel, self).__init__()\n",
    "        # Model layers and components go here\n",
    "        # e.g., self.conv1 = nn.Conv2d(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        # Example of a layer passing through\n",
    "        x = self.conv1(x)  # Example layer\n",
    "        print(f\"After conv1: {x.shape}\")\n",
    "        \n",
    "        # Continue with the rest of the layers\n",
    "        x = self.fc1(x)  # Example fully connected layer\n",
    "        print(f\"After fc1: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=816\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from ast_models import ASTModel \n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = ViolinAudioDataset(json_file=\"data_colab.json\", num_classes=NUM_CLASSES)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model and optimizer (make sure your AST model is properly defined)\n",
    "model = ASTModel(label_dim=num_classes, fstride=10, tstride=10, input_fdim=128, input_tdim=max_time_steps, \n",
    "                 audioset_pretrain=False)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005) \n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training Parameters (with weighted loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=816\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from ast_models import ASTModel \n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Calculate pos_weight for each class\n",
    "num_samples_per_class = [1106 for i in range(NUM_CLASSES)]     #number of items in train_data.json\n",
    "num_positives_per_class = [175, 30, 31, 30, 26, 92, 184, 285, 113, 113, 134, 41, 31, 32, 30, 28, 61, 216, 95, 24] #taken from Code(database)\n",
    "scaling_factor = 2\n",
    "# Compute pos_weight\n",
    "pos_weight = torch.tensor(\n",
    "    [num_samples / num_positives / scaling_factor for num_samples, num_positives in zip(num_samples_per_class, num_positives_per_class)],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "num_epochs = 5\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = ViolinAudioDataset(json_file=\"train_data.json\", num_classes=num_classes)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model and optimizer (make sure your AST model is properly defined)\n",
    "model = ASTModel(label_dim=num_classes, fstride=10, tstride=10, input_fdim=128, input_tdim=max_time_steps, \n",
    "                 audioset_pretrain=False)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001) \n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Epoch 1/5\n",
      "Finished Epoch 1/5, Loss: 0.7160080351056717\n",
      "Beginning Epoch 2/5\n",
      "Finished Epoch 2/5, Loss: 0.5517363650943229\n",
      "Beginning Epoch 3/5\n",
      "Finished Epoch 3/5, Loss: 0.46779788982722686\n",
      "Beginning Epoch 4/5\n",
      "Finished Epoch 4/5, Loss: 0.3912559452838859\n",
      "Beginning Epoch 5/5\n",
      "Finished Epoch 5/5, Loss: 0.30551777846410627\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f\"Beginning Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    for mel_spectrogram, labels in train_loader:\n",
    "        mel_spectrogram, labels = mel_spectrogram.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mel_spectrogram)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Finished Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training (with Gradient Accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Epoch 1/5\n",
      "Finished Epoch 1/5, Loss: 0.7457695109314988\n",
      "Beginning Epoch 2/5\n",
      "Finished Epoch 2/5, Loss: 0.5753985324615165\n",
      "Beginning Epoch 3/5\n",
      "Finished Epoch 3/5, Loss: 0.4952367063456806\n",
      "Beginning Epoch 4/5\n",
      "Finished Epoch 4/5, Loss: 0.4231891400815673\n",
      "Beginning Epoch 5/5\n",
      "Finished Epoch 5/5, Loss: 0.3416443977801375\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "accumulation_steps = 2  # Number of steps to accumulate gradients before updating weights\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f\"Beginning Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Clear gradients at the start of each epoch\n",
    "    \n",
    "    for batch_idx, (mel_spectrogram, labels) in enumerate(train_loader):\n",
    "        mel_spectrogram, labels = mel_spectrogram.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(mel_spectrogram)  # Explicit call to forward with debugging\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss = loss / accumulation_steps  # Scale loss for accumulation\n",
    "        loss.backward()  # Backward pass (accumulate gradients)\n",
    "\n",
    "        # optimizer updates weights only after 4 steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Clear gradients after the step\n",
    "            \n",
    "        total_loss += loss.item() * accumulation_steps  # Undo scaling for correct total loss\n",
    "\n",
    "     # Handle leftover gradients if the dataset size is not divisible by accumulation_steps\n",
    "    if (batch_idx + 1) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"Finished Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prodigy Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=816\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from prodigyopt import Prodigy\n",
    "from ast_models import ASTModel\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 20\n",
    "batch_size = 1\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = ViolinAudioDataset(json_file=\"data_colab.json\", num_classes=num_classes)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model and optimizer (make sure your AST model is properly defined)\n",
    "model = ASTModel(label_dim=num_classes, fstride=10, tstride=10, input_fdim=128, input_tdim=688, \n",
    "                 audioset_pretrain=False)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Prodigy(model.parameters(), lr=0.00005)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Epoch 1/5\n",
      "Finished Epoch 1/5, Loss: 0.6333688658299174\n",
      "Beginning Epoch 2/5\n",
      "Finished Epoch 2/5, Loss: 0.6333136197801784\n",
      "Beginning Epoch 3/5\n",
      "Finished Epoch 3/5, Loss: 0.6332827596154147\n",
      "Beginning Epoch 4/5\n",
      "Finished Epoch 4/5, Loss: 0.6332504991940286\n",
      "Beginning Epoch 5/5\n",
      "Finished Epoch 5/5, Loss: 0.633215740204547\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Beginning Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    for mel_spectrogram, labels in train_loader:\n",
    "        mel_spectrogram, labels = mel_spectrogram.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(f\"Batch shape: {mel_spectrogram.shape}\")\n",
    "        #outputs = model.forward(mel_spectrogram)\n",
    "        outputs = model(mel_spectrogram)  # Explicit call to forward with debugging\n",
    "        #print(f\"Output shape: {outputs.shape}\")\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Finished Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Metrics of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3610\n",
      "Precision: 0.6237\n",
      "Recall: 0.8712\n",
      "F1-Score: 0.7021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for mel_spectrogram, labels in data_loader:\n",
    "            mel_spectrogram = mel_spectrogram.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(mel_spectrogram)\n",
    "            predictions = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
    "\n",
    "            # Threshold probabilities to get binary predictions\n",
    "            predictions = (predictions > 0.5).float()\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='samples')\n",
    "    recall = recall_score(all_labels, all_predictions, average='samples')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='samples')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Load validation/test dataset\n",
    "test_dataset = ViolinAudioDataset(json_file=\"test_data.json\", num_classes=num_classes)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ast_model_new.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"ast_model_new.pth\"  # Specify the path where you want to save the model\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 688])\n",
      "Predicted Labels: [0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "True Labels:      [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def test_single_sample(model, sample, device, threshold=0.5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        # Move sample to device\n",
    "        sample = sample.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(sample.unsqueeze(0))  # Add batch dimension\n",
    "        probabilities = torch.sigmoid(output)  # Convert logits to probabilities\n",
    "\n",
    "        # Threshold probabilities to get binary predictions\n",
    "        binary_predictions = (probabilities > threshold).float()\n",
    "\n",
    "    return binary_predictions.squeeze(0).cpu().numpy()  # Remove batch dimension and move to CPU\n",
    "\n",
    "\n",
    "# Get a single sample from the DataLoader\n",
    "for mel_spectrogram, labels in train_loader:\n",
    "    sample_mel_spectrogram = mel_spectrogram[0]  # Get the first sample\n",
    "    sample_label = labels[0]  # Get the corresponding label\n",
    "    break\n",
    "    \n",
    "# Test the model on a single sample\n",
    "predictions = test_single_sample(model, sample_mel_spectrogram, device)\n",
    "\n",
    "# Print results\n",
    "print(sample_mel_spectrogram.shape)\n",
    "print(\"Predicted Labels:\", predictions)\n",
    "print(\"True Labels:     \", sample_label.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
